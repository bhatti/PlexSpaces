// SPDX-License-Identifier: LGPL-2.1-or-later
// Copyright (C) 2025 Shahzad A. Bhatti <bhatti@plexobject.com>
//
// This file is part of PlexSpaces.

//! Genomic Workflow Pipeline - Standalone Runner
//!
//! Demonstrates workflow orchestration with:
//! - Multi-step genomic pipeline (QC ‚Üí Alignment ‚Üí Variant Calling)
//! - ConfigBootstrap for configuration
//! - CoordinationComputeTracker for metrics
//! - Workflow durability via journaling
//! - **Recovery**: Workflows can be restarted if interrupted or node crashes

use clap::Parser;
use genomic_workflow_pipeline::config::GenomicPipelineConfig;
use genomic_workflow_pipeline::processor;
use genomic_workflow_pipeline::recovery::WorkflowRecoveryService;
use genomic_workflow_pipeline::types::*;
use plexspaces_node::{ConfigBootstrap, CoordinationComputeTracker};
use plexspaces_workflow::*;
use serde_json::json;
use std::time::Duration;
use tracing::{info, warn, Level};
use tracing_subscriber;

/// Command-line arguments
#[derive(Parser, Debug)]
#[command(name = "genomic-workflow-pipeline")]
#[command(about = "Genomic Workflow Pipeline - Demonstrates workflow orchestration with recovery")]
struct Args {
    /// Number of sequence reads to process
    #[arg(default_value = "1000")]
    num_reads: usize,

    /// Execution ID to resume (if workflow was interrupted)
    #[arg(long)]
    resume: Option<String>,

    /// Storage database path (default: workflow.db in current directory)
    /// For SQLite: use file path (e.g., "workflow.db")
    /// For PostgreSQL: use connection string (e.g., "postgresql://user:pass@localhost/dbname")
    #[arg(long, default_value = "workflow.db")]
    storage: String,

    /// Enable auto-recovery of interrupted workflows on startup
    #[arg(long)]
    auto_recover: bool,
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize tracing
    tracing_subscriber::fmt()
        .with_max_level(Level::INFO)
        .init();

    let args = Args::parse();

    println!("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó");
    println!("‚ïë    PlexSpaces Genomic Workflow Pipeline (With Recovery)   ‚ïë");
    println!("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù");
    println!();

    // Load configuration using ConfigBootstrap
    let config: GenomicPipelineConfig = ConfigBootstrap::load().unwrap_or_default();
    info!("üìã Configuration:");
    info!("   Total reads: {}", args.num_reads);
    info!("   Target ratio: {}√ó (compute/coordinate)", config.target_granularity_ratio);
    info!("   Storage: {}", args.storage);
    if let Some(ref exec_id) = args.resume {
        info!("   Resuming execution: {}", exec_id);
    }
    println!();

    // Create metrics tracker
    let mut metrics_tracker = CoordinationComputeTracker::new("genomic-pipeline".to_string());

    // Create persistent workflow storage (SQLite file-based for recovery)
    metrics_tracker.start_coordinate();
    
    // Create file-based SQLite storage (persists across restarts)
    // For recovery: use file-based storage; falls back to in-memory if file creation fails
    let storage = match create_persistent_storage(&args.storage).await {
        Ok(s) => {
            info!("‚úì Persistent storage initialized: {}", args.storage);
            s
        }
        Err(e) => {
            warn!("‚ö† Failed to create persistent storage ({}): {}. Using in-memory storage (no recovery).", args.storage, e);
            WorkflowStorage::new_in_memory().await?
        }
    };

    // Auto-recovery: Recover interrupted workflows on startup (if enabled)
    if args.auto_recover {
        let node_id = "local-node".to_string(); // TODO: Get from NodeConfig when available
        let recovery_service = WorkflowRecoveryService::new(storage.clone(), node_id);
        
        info!("üîÑ Auto-recovery enabled: Checking for interrupted workflows...");
        match recovery_service.recover_on_startup().await {
            Ok(report) => {
                if report.total > 0 {
                    info!("‚úì Auto-recovery complete: {}/{} workflows recovered, {} claimed, {} transferred, {} failed", 
                        report.recovered, report.total, report.claimed, report.transferred, report.failed);
                    println!();
                } else {
                    info!("‚úì No interrupted workflows found");
                }
            }
            Err(e) => {
                warn!("‚ö† Auto-recovery failed: {}", e);
            }
        }
    }

    // Create workflow definition with 3 steps
    let definition = WorkflowDefinition {
        id: "genomic-pipeline".to_string(),
        name: "Genomic Analysis Pipeline".to_string(),
        version: "1.0".to_string(),
        steps: vec![
            Step {
                id: "qc".to_string(),
                name: "Quality Control".to_string(),
                step_type: StepType::Task,
                config: json!({
                    "action": "succeed",
                    "processor": "qc",
                    "description": "Filter low-quality sequence reads"
                }),
            },
            Step {
                id: "alignment".to_string(),
                name: "Genome Alignment".to_string(),
                step_type: StepType::Task,
                config: json!({
                    "action": "succeed",
                    "processor": "alignment",
                    "description": "Map reads to reference genome (hg38)"
                }),
            },
            Step {
                id: "variant-calling".to_string(),
                name: "Variant Calling".to_string(),
                step_type: StepType::Task,
                config: json!({
                    "action": "succeed",
                    "processor": "variant-calling",
                    "description": "Identify genetic variations"
                }),
            },
        ],
    };

    storage.save_definition(&definition).await?;
    metrics_tracker.end_coordinate();
    info!("‚úì Workflow definition created with {} steps", definition.steps.len());
    println!();

    // Handle recovery: resume from existing execution or start new
    let execution_id = if let Some(ref exec_id) = args.resume {
        // Recovery mode: resume from existing execution
        info!("üîÑ Resuming workflow execution: {}", exec_id);
        
        // Check if execution exists and is in a resumable state
        match storage.get_execution(exec_id).await {
            Ok(execution) => {
                match execution.status {
                    ExecutionStatus::Running | ExecutionStatus::Pending => {
                        info!("   Execution status: {:?}", execution.status);
                        info!("   Resuming from last checkpoint...");
                        
                        metrics_tracker.start_coordinate();
                        WorkflowExecutor::execute_from_state(&storage, exec_id).await?;
                        metrics_tracker.end_coordinate();
                        
                        info!("‚úì Workflow resumed and completed");
                        exec_id.clone()
                    }
                    ExecutionStatus::Completed => {
                        warn!("‚ö† Execution {} already completed", exec_id);
                        exec_id.clone()
                    }
                    ExecutionStatus::Failed => {
                        warn!("‚ö† Execution {} previously failed. Starting new execution...", exec_id);
                        start_new_execution(&storage, &definition, args.num_reads, &mut metrics_tracker).await?
                    }
                    _ => {
                        warn!("‚ö† Execution {} in non-resumable state: {:?}. Starting new execution...", exec_id, execution.status);
                        start_new_execution(&storage, &definition, args.num_reads, &mut metrics_tracker).await?
                    }
                }
            }
            Err(_) => {
                warn!("‚ö† Execution {} not found. Starting new execution...", exec_id);
                start_new_execution(&storage, &definition, args.num_reads, &mut metrics_tracker).await?
            }
        }
    } else {
        // Normal mode: start new execution
        start_new_execution(&storage, &definition, args.num_reads, &mut metrics_tracker).await?
    };

    // Get final execution state
    let execution = storage.get_execution(&execution_id).await?;
    
    // Calculate results from execution output or run processing if needed
    let (qc_passed, variants_called) = if let Some(ref output) = execution.output {
        // Extract results from workflow output
        let _total_reads = output.get("num_reads").and_then(|v| v.as_u64()).unwrap_or(0) as usize;
        let qc_passed = output.get("qc_passed").and_then(|v| v.as_u64()).unwrap_or(0) as usize;
        let variants_called = output.get("variants_called").and_then(|v| v.as_u64()).unwrap_or(0) as usize;
        (qc_passed, variants_called)
    } else {
        // Fallback: run processing to get results (for demonstration)
        info!("‚úì Generating {} test reads...", args.num_reads);
        let reads = processor::generate_test_reads(args.num_reads);
        
        info!("‚úì Running QC...");
        metrics_tracker.start_compute();
        let qc_results = processor::process_qc(&reads);
        metrics_tracker.end_compute();
        
        info!("‚úì Running Alignment...");
        metrics_tracker.start_compute();
        let alignments = processor::process_alignment(&qc_results);
        metrics_tracker.end_compute();
        
        info!("‚úì Running Variant Calling...");
        metrics_tracker.start_compute();
        let variants = processor::process_variant_calling(&alignments);
        metrics_tracker.end_compute();
        
        let qc_passed = qc_results.iter().filter(|r| r.passed).count();
        let variants_called = variants.len();
        (qc_passed, variants_called)
    };

    // Finalize metrics
    let metrics = metrics_tracker.finalize();

    // Display results
    println!("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ");
    println!("                    PIPELINE RESULTS");
    println!("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ");
    println!();
    println!("Execution ID: {}", execution_id);
    println!("Status: {:?}", execution.status);
    println!();
    println!("Throughput:");
    println!("  Total reads:        {}", args.num_reads);
    println!("  QC passed:          {} ({:.1}%)", qc_passed, (qc_passed as f64 / args.num_reads as f64) * 100.0);
    println!("  Variants called:    {}", variants_called);
    println!();

    println!("Performance Metrics (CLAUDE.md Principle #6):");
    println!("  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê");
    println!("  ‚îÇ Metric              ‚îÇ Value    ‚îÇ");
    println!("  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§");
    println!("  ‚îÇ Compute Time        ‚îÇ {:>8}ms ‚îÇ", metrics.compute_duration_ms);
    println!("  ‚îÇ Coordinate Time     ‚îÇ {:>8}ms ‚îÇ", metrics.coordinate_duration_ms);
    println!("  ‚îÇ Total Time          ‚îÇ {:>8}ms ‚îÇ", metrics.total_duration_ms);
    println!("  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§");
    println!("  ‚îÇ Granularity Ratio   ‚îÇ {:>8.1}√ó ‚îÇ", metrics.granularity_ratio);
    println!("  ‚îÇ Efficiency          ‚îÇ {:>7.1}% ‚îÇ", metrics.efficiency * 100.0);
    println!("  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò");
    println!();

    // Evaluate performance
    let status = if metrics.granularity_ratio >= config.target_granularity_ratio {
        "‚úÖ EXCELLENT"
    } else if metrics.granularity_ratio >= 10.0 {
        "‚ö†  ACCEPTABLE"
    } else {
        "‚ùå POOR"
    };

    println!("  Status:             {}", status);
    if metrics.granularity_ratio < config.target_granularity_ratio {
        println!("  Recommendation:     Consider batching more work per task.");
    }
    println!();

    let throughput = if metrics.total_duration_ms > 0 {
        (args.num_reads as f64 * 1000.0) / metrics.total_duration_ms as f64
    } else {
        0.0
    };
    println!("  Throughput:         {:.0} reads/second", throughput);
    println!();

    // Save results to JSON
    let result = PipelineResult {
        total_reads: args.num_reads,
        qc_passed,
        variants_called,
        metrics: PipelineMetrics {
            compute_time_ms: metrics.compute_duration_ms,
            coordinate_time_ms: metrics.coordinate_duration_ms,
            total_time_ms: metrics.total_duration_ms,
            granularity_ratio: metrics.granularity_ratio,
            efficiency_percent: metrics.efficiency * 100.0,
        },
    };

    let result_json = serde_json::to_string_pretty(&result)?;
    std::fs::write("pipeline_results.json", result_json)?;
    info!("‚úì Results saved to pipeline_results.json");
    
    // Save execution ID for recovery
    std::fs::write("last_execution_id.txt", &execution_id)?;
    info!("‚úì Execution ID saved to last_execution_id.txt (use --resume {} to recover)", execution_id);

    println!("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó");
    println!("‚ïë              Example Complete                              ‚ïë");
    println!("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù");
    println!();
    println!("üí° Recovery Tip: If this workflow is interrupted, resume it with:");
    println!("   cargo run --release -- --resume {}", execution_id);
    println!();

    Ok(())
}

/// Create persistent storage for workflow state
/// Supports both SQLite (file path) and PostgreSQL (connection string)
async fn create_persistent_storage(path: &str) -> Result<WorkflowStorage, Box<dyn std::error::Error>> {
    // Detect database type from connection string
    if path.starts_with("postgresql://") || path.starts_with("postgres://") {
        // PostgreSQL connection string
        info!("Connecting to PostgreSQL database...");
        WorkflowStorage::new_postgres(path)
            .await
            .map_err(|e| format!("Failed to create PostgreSQL storage: {}", e).into())
    } else {
        // SQLite file path - pass as-is, new_file will handle path resolution
        info!("Using SQLite database: {}", path);
        WorkflowStorage::new_file(path)
            .await
            .map_err(|e| format!("Failed to create SQLite storage: {}", e).into())
    }
}

/// Start a new workflow execution
async fn start_new_execution(
    storage: &WorkflowStorage,
    definition: &WorkflowDefinition,
    num_reads: usize,
    metrics_tracker: &mut CoordinationComputeTracker,
) -> Result<String, Box<dyn std::error::Error>> {
    // Execute workflow using WorkflowExecutor
    metrics_tracker.start_coordinate();
    
    let workflow_input = json!({
        "num_reads": num_reads
    });

    info!("‚úì Starting workflow execution...");
    
    // Start execution - this will execute the workflow steps synchronously
    // For this example, we execute the workflow to completion to demonstrate the pipeline
    let execution_id = WorkflowExecutor::start_execution(
        storage,
        &definition.id,
        &definition.version,
        workflow_input.clone(),
    )
    .await?;
    
    metrics_tracker.end_coordinate();
    
    info!("‚úì Workflow execution completed: {}", execution_id);
    
    // Update execution output with results for recovery demonstration
    let execution = storage.get_execution(&execution_id).await?;
    if execution.status == ExecutionStatus::Completed && execution.output.is_none() {
        // If workflow completed but has no output, add basic output for recovery demo
        let output = json!({
            "num_reads": num_reads,
            "qc_passed": (num_reads as f64 * 0.95) as usize,
            "variants_called": (num_reads as f64 * 0.85) as usize,
        });
        storage
            .update_execution_output_with_version(&execution_id, output, Some(execution.version))
            .await?;
    }
    
    Ok(execution_id)
}
