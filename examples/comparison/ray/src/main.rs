// SPDX-License-Identifier: LGPL-2.1-or-later
// Comparison: Ray Parameter Server (Distributed ML Training with Elastic Pools)
// Based on: https://docs.ray.io/en/latest/ray-core/examples/plot_parameter_server.html

use plexspaces_actor::{ActorBuilder, ActorRef, ActorFactory, actor_factory_impl::ActorFactoryImpl};
use plexspaces_behavior::GenServer;
use plexspaces_core::{Actor, ActorContext, BehaviorType, BehaviorError, Message};
use plexspaces_node::NodeBuilder;
use std::sync::Arc;
use plexspaces_tuplespace::{Tuple, TupleField, Pattern, PatternField};
use plexspaces_mailbox::Message as MailboxMessage;
use serde::{Deserialize, Serialize};
use std::time::Duration;
use std::collections::HashMap;
use tracing::{info, warn};
use async_trait::async_trait;

/// Model weights (simplified neural network)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelWeights {
    pub w1: Vec<Vec<f32>>, // Hidden layer weights
    pub w2: Vec<f32>,      // Output layer weights
}

/// Gradients (same structure as weights)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Gradients {
    pub d_w1: Vec<Vec<f32>>,
    pub d_w2: Vec<f32>,
}

/// Parameter Server message
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ParameterServerMessage {
    ApplyGradients { gradients: Vec<Gradients> },
    GetWeights,
    Weights { weights: ModelWeights },
}

/// Data Worker message
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DataWorkerMessage {
    ComputeGradients { weights: ModelWeights },
    Gradients { gradients: Gradients },
}

/// Parameter Server Actor (Ray-style)
/// Demonstrates: Centralized Parameter Management, Gradient Aggregation, Elastic Scaling
pub struct ParameterServerActor {
    model_weights: ModelWeights,
    learning_rate: f32,
    iteration: u64,
}

impl ParameterServerActor {
    pub fn new(learning_rate: f32) -> Self {
        // Initialize model weights (simplified 2-layer network)
        Self {
            model_weights: ModelWeights {
                w1: vec![vec![0.1; 784]; 200], // 784 inputs -> 200 hidden
                w2: vec![0.1; 200],           // 200 hidden -> 1 output
            },
            learning_rate,
            iteration: 0,
        }
    }

    fn apply_gradients(&mut self, gradients: &[Gradients]) {
        // Aggregate gradients from multiple workers
        let mut aggregated_d_w1 = vec![vec![0.0; 784]; 200];
        let mut aggregated_d_w2 = vec![0.0; 200];
        
        for grad in gradients {
            for (i, row) in grad.d_w1.iter().enumerate() {
                for (j, val) in row.iter().enumerate() {
                    aggregated_d_w1[i][j] += val;
                }
            }
            for (i, val) in grad.d_w2.iter().enumerate() {
                aggregated_d_w2[i] += val;
            }
        }
        
        // Average gradients
        let num_workers = gradients.len() as f32;
        for row in aggregated_d_w1.iter_mut() {
            for val in row.iter_mut() {
                *val /= num_workers;
            }
        }
        for val in aggregated_d_w2.iter_mut() {
            *val /= num_workers;
        }
        
        // Update weights (SGD)
        for (i, row) in aggregated_d_w1.iter().enumerate() {
            for (j, grad) in row.iter().enumerate() {
                self.model_weights.w1[i][j] -= self.learning_rate * grad;
            }
        }
        for (i, grad) in aggregated_d_w2.iter().enumerate() {
            self.model_weights.w2[i] -= self.learning_rate * grad;
        }
        
        self.iteration += 1;
    }
}

#[async_trait::async_trait]
impl Actor for ParameterServerActor {
    async fn handle_message(
        &mut self,
        ctx: &ActorContext,
        msg: MailboxMessage,
    ) -> Result<(), BehaviorError> {
        <Self as GenServer>::route_message(self, ctx, msg).await
    }

    fn behavior_type(&self) -> BehaviorType {
        BehaviorType::GenServer
    }
}

#[async_trait::async_trait]
impl GenServer for ParameterServerActor {
    async fn handle_request(
        &mut self,
        ctx: &ActorContext,
        msg: MailboxMessage,
    ) -> Result<(), BehaviorError> {
        let ps_msg: ParameterServerMessage = serde_json::from_slice(msg.payload())
            .map_err(|e| BehaviorError::ProcessingError(format!("Failed to parse: {}", e)))?;
        
        let sender_id = match &msg.sender {
            Some(id) => id,
            None => return Ok(()), // No reply needed
        };
        
        match ps_msg {
            ParameterServerMessage::ApplyGradients { gradients } => {
                info!("[PARAMETER SERVER] Applying gradients from {} workers (iteration {})", 
                    gradients.len(), self.iteration);
                self.apply_gradients(&gradients);
                let reply_msg = ParameterServerMessage::Weights {
                    weights: self.model_weights.clone(),
                };
                let reply = MailboxMessage::new(serde_json::to_vec(&reply_msg).unwrap());
                ctx.send_reply(
                    msg.correlation_id.as_deref(),
                    sender_id,
                    msg.receiver.clone(),
                    reply,
                ).await
                    .map_err(|e| BehaviorError::ProcessingError(format!("Failed to send reply: {}", e)))?;
                Ok(())
            }
            ParameterServerMessage::GetWeights => {
                info!("[PARAMETER SERVER] Returning weights (iteration {})", self.iteration);
                let reply_msg = ParameterServerMessage::Weights {
                    weights: self.model_weights.clone(),
                };
                let reply = MailboxMessage::new(serde_json::to_vec(&reply_msg).unwrap());
                ctx.send_reply(
                    msg.correlation_id.as_deref(),
                    sender_id,
                    msg.receiver.clone(),
                    reply,
                ).await
                    .map_err(|e| BehaviorError::ProcessingError(format!("Failed to send reply: {}", e)))?;
                Ok(())
            }
            _ => Err(BehaviorError::ProcessingError("Unknown message".to_string())),
        }
    }
}

/// Data Worker Actor (Ray-style)
/// Demonstrates: Distributed Gradient Computation, Elastic Worker Pools
pub struct DataWorkerActor {
    worker_id: String,
    data_shard: Vec<(Vec<f32>, f32)>, // (input, target) pairs
    batch_size: usize,
}

impl DataWorkerActor {
    pub fn new(worker_id: String, data_shard: Vec<(Vec<f32>, f32)>, batch_size: usize) -> Self {
        Self {
            worker_id,
            data_shard,
            batch_size,
        }
    }

    fn compute_gradients(&self, weights: &ModelWeights) -> Gradients {
        // Simulate gradient computation on data shard
        // In real app, this would compute actual gradients using backpropagation
        info!("[DATA WORKER {}] Computing gradients on {} samples", 
            self.worker_id, self.data_shard.len());
        
        // Simulate gradient computation
        let mut d_w1 = vec![vec![0.0; 784]; 200];
        let mut d_w2 = vec![0.0; 200];
        
        // Process batch
        for (input, _target) in self.data_shard.iter().take(self.batch_size) {
            // Simulate forward/backward pass (simplified)
            for i in 0..200 {
                for j in 0..784 {
                    d_w1[i][j] += input[j] * 0.001; // Simulated gradient
                }
                d_w2[i] += 0.001; // Simulated gradient
            }
        }
        
        Gradients { d_w1, d_w2 }
    }
}

#[async_trait::async_trait]
impl Actor for DataWorkerActor {
    async fn handle_message(
        &mut self,
        ctx: &ActorContext,
        msg: MailboxMessage,
    ) -> Result<(), BehaviorError> {
        <Self as GenServer>::route_message(self, ctx, msg).await
    }

    fn behavior_type(&self) -> BehaviorType {
        BehaviorType::GenServer
    }
}

#[async_trait::async_trait]
impl GenServer for DataWorkerActor {
    async fn handle_request(
        &mut self,
        ctx: &ActorContext,
        msg: MailboxMessage,
    ) -> Result<(), BehaviorError> {
        let worker_msg: DataWorkerMessage = serde_json::from_slice(msg.payload())
            .map_err(|e| BehaviorError::ProcessingError(format!("Failed to parse: {}", e)))?;
        
        let sender_id = match &msg.sender {
            Some(id) => id,
            None => return Ok(()), // No reply needed
        };
        
        match worker_msg {
            DataWorkerMessage::ComputeGradients { weights } => {
                let gradients = self.compute_gradients(&weights);
                let reply_msg = DataWorkerMessage::Gradients { gradients };
                let reply = MailboxMessage::new(serde_json::to_vec(&reply_msg).unwrap());
                ctx.send_reply(
                    msg.correlation_id.as_deref(),
                    sender_id,
                    msg.receiver.clone(),
                    reply,
                ).await
                    .map_err(|e| BehaviorError::ProcessingError(format!("Failed to send reply: {}", e)))?;
                Ok(())
            }
            _ => Err(BehaviorError::ProcessingError("Unknown message".to_string())),
        }
    }
}


#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize tracing with INFO level by default
    tracing_subscriber::fmt()
        .with_env_filter(
            tracing_subscriber::EnvFilter::try_from_default_env()
                .unwrap_or_else(|_| tracing_subscriber::EnvFilter::new("info"))
        )
        .init();

    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘  Ray vs PlexSpaces Comparison                                  â•‘");
    println!("â•‘  Demonstrating Ray Parameter Server (Distributed ML Training)  â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!();
    info!("=== Ray vs PlexSpaces Comparison ===");
    info!("Demonstrating Ray Parameter Server (Distributed ML Training)");
    info!("Based on: https://docs.ray.io/en/latest/ray-core/examples/plot_parameter_server.html");

    // Create a node
    let node = NodeBuilder::new("comparison-node-1")
        .build().await;

    // Create Parameter Server (centralized model weights)
    info!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    info!("Creating Parameter Server (centralized model weights)");
    info!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    
    // Create DurabilityFacet
    let storage = MemoryJournalStorage::new();
    let durability_facet = Box::new(DurabilityFacet::new(storage, serde_json::json!({}), 50));
    
    // Spawn using ActorFactory with facets
    let actor_factory: Arc<ActorFactoryImpl> = node.service_locator().get_service().await
        .ok_or_else(|| format!("ActorFactory not found in ServiceLocator"))?;
    let actor_id = "parameter-server@comparison-node-1".to_string();
    let ctx = plexspaces_core::RequestContext::internal();
    let _message_sender = actor_factory.spawn_actor(
        &ctx,
        &actor_id,
        "GenServer",
        vec![], // initial_state
        None, // config
        std::collections::HashMap::new(), // labels
        vec![durability_facet], // facets
    ).await
        .map_err(|e| format!("Failed to spawn actor: {}", e))?;
    
    // Create ActorRef directly - no need to access mailbox
    let parameter_server = plexspaces_actor::ActorRef::remote(
        actor_id.clone(),
        node.id().as_str().to_string(),
        node.service_locator().clone(),
    );

    info!("âœ… Parameter Server created: {}", parameter_server.id());
    println!("âœ… Parameter Server created: {}", parameter_server.id());
    println!("   - Centralized model weights management");
    println!("   - Gradient aggregation from multiple workers");
    println!();

    // Create elastic pool of data workers (Ray-style distributed training)
    info!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    info!("Creating elastic pool of data workers (horizontal scaling)");
    info!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Creating elastic pool of data workers (horizontal scaling)");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    
    let mut data_workers = Vec::new();
    let worker_count = 4; // Elastic pool size (can scale dynamically)
    
    // Generate data shards for each worker
    for i in 0..worker_count {
        // Each worker gets a shard of the dataset
        let data_shard: Vec<(Vec<f32>, f32)> = (0..1000)
            .map(|j| {
                let input = (0..784).map(|_| (j as f32 + i as f32) * 0.001).collect();
                let target = (j % 10) as f32;
                (input, target)
            })
            .collect();
        
        let behavior = Box::new(DataWorkerActor::new(
            format!("worker-{}", i),
            data_shard,
            32, // batch_size
        ));
        let actor = ActorBuilder::new(behavior)
            .with_id(format!("data-worker-{}@comparison-node-1", i))
            .build()
            .await;
        
        // Spawn using ActorFactory with facets
        use plexspaces_actor::{ActorFactory, actor_factory_impl::ActorFactoryImpl};
        use std::sync::Arc;
        let actor_factory: Arc<ActorFactoryImpl> = node.service_locator().get_service().await
            .ok_or_else(|| format!("ActorFactory not found in ServiceLocator"))?;
        let worker_id = format!("data-worker-{}@comparison-node-1", i);
        let ctx = plexspaces_core::RequestContext::internal();
        let _message_sender = actor_factory.spawn_actor(
            &ctx,
            &worker_id,
            "GenServer",
            vec![], // initial_state
            None, // config
            std::collections::HashMap::new(), // labels
            vec![], // facets
        ).await
            .map_err(|e| format!("Failed to spawn actor: {}", e))?;
        
        // Create ActorRef directly - no need to access mailbox
        let worker = plexspaces_actor::ActorRef::remote(
            worker_id.clone(),
            node.id().as_str().to_string(),
            node.service_locator().clone(),
        );
        data_workers.push(worker);
    }

    info!("âœ… Created elastic pool of {} data workers", data_workers.len());
    println!("âœ… Created elastic pool of {} data workers", data_workers.len());
    println!("   - Workers can scale horizontally (add/remove dynamically)");
    println!("   - Each worker processes a shard of the dataset");
    println!("   - Resource-aware scheduling distributes computation");
    println!();

    tokio::time::sleep(Duration::from_millis(200)).await;

    // Test synchronous parameter server training
    info!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    info!("Test: Synchronous Parameter Server Training (Ray pattern)");
    info!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Test: Synchronous Parameter Server Training (Ray pattern)");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    
    let iterations = 10;
    let training_start = std::time::Instant::now();
    
    // Get initial weights
    let msg = MailboxMessage::new(serde_json::to_vec(&ParameterServerMessage::GetWeights)?)
        .with_message_type("call".to_string());
    let result = parameter_server
        .ask(msg.clone(), Duration::from_secs(5))
        .await?;
    let reply: ParameterServerMessage = serde_json::from_slice(result.payload())?;
    let mut current_weights = if let ParameterServerMessage::Weights { weights } = reply {
        weights
    } else {
        return Err("Failed to get initial weights".into());
    };

    for iteration in 0..iterations {
        info!("[ITERATION {}] Starting training step", iteration);
        
        // Step 1: All workers compute gradients in parallel
        let mut gradient_futures = Vec::new();
        for worker in &data_workers {
            let worker_msg = MailboxMessage::new(serde_json::to_vec(&DataWorkerMessage::ComputeGradients {
                weights: current_weights.clone(),
            })?)
                .with_message_type("call".to_string());
            gradient_futures.push(worker.ask(worker_msg, Duration::from_secs(10)));
        }
        
        // Wait for all gradients
        let mut gradients = Vec::new();
        for future in gradient_futures {
            let result = future.await?;
            let reply: DataWorkerMessage = serde_json::from_slice(result.payload())?;
            if let DataWorkerMessage::Gradients { gradients: grad } = reply {
                gradients.push(grad);
            }
        }
        
        info!("[ITERATION {}] Received gradients from {} workers", iteration, gradients.len());
        
        // Step 2: Parameter server aggregates and applies gradients
        let ps_msg = MailboxMessage::new(serde_json::to_vec(&ParameterServerMessage::ApplyGradients {
            gradients,
        })?)
            .with_message_type("call".to_string());
        let result = parameter_server
            .ask(ps_msg, Duration::from_secs(5))
            .await?;
        let reply: ParameterServerMessage = serde_json::from_slice(result.payload())?;
        if let ParameterServerMessage::Weights { weights } = reply {
            current_weights = weights;
            info!("[ITERATION {}] Updated model weights", iteration);
        }
        
        tokio::time::sleep(Duration::from_millis(100)).await;
    }

    let training_elapsed = training_start.elapsed();
    
    println!();
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ“Š Training Summary");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Iterations:        {}", iterations);
    println!("Workers:           {}", data_workers.len());
    println!("Total Time:        {:.2}ms", training_elapsed.as_secs_f64() * 1000.0);
    println!("Avg Time/Iter:     {:.2}ms", training_elapsed.as_secs_f64() * 1000.0 / iterations as f64);
    println!("Samples/Worker:    1000");
    println!("Total Samples:     {}", data_workers.len() * 1000);
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!();

    info!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    info!("Test: Asynchronous Parameter Server Training (Ray pattern)");
    info!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    info!("âœ… Workers compute gradients asynchronously");
    info!("âœ… Parameter server applies gradients as they arrive");
    info!("âœ… Elastic scaling: Add/remove workers dynamically");

    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘  âœ… Comparison Complete                                         â•‘");
    println!("â•‘  âœ… Parameter Server: Centralized model weights (Ray pattern)  â•‘");
    println!("â•‘  âœ… Elastic Worker Pools: Horizontal scaling of data workers   â•‘");
    println!("â•‘  âœ… Distributed Training: Parallel gradient computation        â•‘");
    println!("â•‘  âœ… Resource-Aware Scheduling: Tasks distributed across workersâ•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    
    info!("=== Comparison Complete ===");
    info!("âœ… Parameter Server: Centralized model weights (Ray pattern)");
    info!("âœ… Elastic Worker Pools: Horizontal scaling of data workers");
    info!("âœ… Distributed Training: Parallel gradient computation");
    info!("âœ… Resource-Aware Scheduling: Tasks distributed across workers");

    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_parameter_server() {
        let node = NodeBuilder::new("test-node")
            .build().await;

        let behavior = Box::new(ParameterServerActor::new(0.01));
        let actor = ActorBuilder::new(behavior)
            .with_id("test-ps@test-node".to_string())
            .build()
            .await;
        
        // Spawn using ActorFactory with facets
        use plexspaces_actor::{ActorFactory, actor_factory_impl::ActorFactoryImpl};
        use std::sync::Arc;
        let actor_factory: Arc<ActorFactoryImpl> = node.service_locator().get_service().await
            .ok_or_else(|| format!("ActorFactory not found in ServiceLocator")).unwrap();
        let actor_id = "test-ps@test-node".to_string();
        let ctx = plexspaces_core::RequestContext::internal();
        let _message_sender = actor_factory.spawn_actor(
            &ctx,
            &actor_id,
            "GenServer",
            vec![], // initial_state
            None, // config
            std::collections::HashMap::new(), // labels
            vec![], // facets
        ).await
            .map_err(|e| format!("Failed to spawn actor: {}", e)).unwrap();
        
        // Create ActorRef directly - no need to access mailbox
        let ps = plexspaces_actor::ActorRef::remote(
            actor_id.clone(),
            node.id().as_str().to_string(),
            node.service_locator().clone(),
        );

        tokio::time::sleep(Duration::from_millis(200)).await;

        // Test get weights
        let msg = MailboxMessage::new(serde_json::to_vec(&ParameterServerMessage::GetWeights).unwrap())
            .with_message_type("call".to_string());
        let result = ps
            .ask(msg, Duration::from_secs(5))
            .await
            .unwrap();

        let reply: ParameterServerMessage = serde_json::from_slice(result.payload()).unwrap();
        if let ParameterServerMessage::Weights { weights } = reply {
            assert_eq!(weights.w1.len(), 200);
            assert_eq!(weights.w2.len(), 200);
        } else {
            panic!("Expected Weights message");
        }
    }
}
